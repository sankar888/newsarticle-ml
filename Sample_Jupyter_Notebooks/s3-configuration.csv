Group,Name,Mandatory,Importance,Default Value,Description
Common,CONNECTOR_NAME,Mandatory,HIGH,,Globally unique name to use for this connector.
Common,CONNECTOR_CONNECTOR_CLASS,Mandatory,HIGH,,"Name or alias of the class for this connector. Must be a subclass of org.apache.kafka.connect.connector.Connector. If the connector is org.apache.kafka.connect.file.FileStreamSinkConnector, you can either specify this full name,  or use FileStreamSink"" or ""FileStreamSinkConnector"" to make the configuration a bit shorter"""
Common,CONNECTOR_TASKS_MAX,Optional,HIGH,1,Maximum number of tasks to use for this connector.
Common,CONNECTOR_KEY_CONVERTER,Optional,LOW,,"Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro."
Common,CONNECTOR_VALUE_CONVERTER,Optional,LOW,,"Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro."
Common,CONNECTOR_HEADER_CONVERTER,Optional,LOW,,"HeaderConverter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the header values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro. By default, the SimpleHeaderConverter is used to serialize header values to strings and deserialize them by inferring the schemas."
Transforms,CONNECTOR_TRANSFORMS,Optional,LOW,,Aliases for the transformations to be applied to records.
Common,CONNECTOR_CONFIG_ACTION_RELOAD,Optional,LOW,RESTART,The action that Connect should take on the connector when changes in external configuration providers result in a change in the connector's configuration properties. A value of 'none' indicates that Connect will do nothing. A value of 'restart' indicates that Connect should restart/reload the connector with the updated configuration properties.The restart may actually be scheduled in the future if the external configuration provider indicates that a configuration value will expire in the future.
Error Handling,CONNECTOR_ERRORS_RETRY_TIMEOUT,Optional,MEDIUM,0,"The maximum duration in milliseconds that a failed operation will be reattempted. The default is 0, which means no retries will be attempted. Use -1 for infinite retries."
Error Handling,CONNECTOR_ERRORS_RETRY_DELAY_MAX_MS,Optional,MEDIUM,60000,The maximum duration in milliseconds between consecutive retry attempts. Jitter will be added to the delay once this limit is reached to prevent thundering herd issues.
Error Handling,CONNECTOR_ERRORS_TOLERANCE,Optional,MEDIUM,none,Behavior for tolerating errors during connector operation. 'none' is the default value and signals that any error will result in an immediate connector task failure; 'all' changes the behavior to skip over problematic records.
Error Handling,CONNECTOR_ERRORS_LOG_ENABLE,Optional,MEDIUM,FALSE,"If true, write each error and the details of the failed operation and problematic record to the Connect application log. This is 'false' by default, so that only errors that are not tolerated are reported."
Error Handling,CONNECTOR_ERRORS_LOG_INCLUDE_MESSAGES,Optional,MEDIUM,FALSE,"Whether to the include in the log the Connect record that resulted in a failure. This is 'false' by default, which will prevent record keys, values, and headers from being written to log files, although some information such as topic and partition number will still be logged."
Common,CONNECTOR_TOPICS,Optional,HIGH,,"List of topics to consume, separated by commas"
Common,CONNECTOR_TOPICS_REGEX,Optional,HIGH,,"Regular expression giving topics to consume. Under the hood, the regex is compiled to a <code>java.util.regex.Pattern</code>. Only one of topics or topics.regex should be specified."
Error Handling,CONNECTOR_ERRORS_DEADLETTERQUEUE_TOPIC_NAME,Optional,MEDIUM,,"The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. The topic name is blank by default, which means that no messages are to be recorded in the DLQ."
Error Handling,CONNECTOR_ERRORS_DEADLETTERQUEUE_TOPIC_REPLICATION_FACTOR,Optional,MEDIUM,3,Replication factor used to create the dead letter queue topic when it doesn't already exist.
Error Handling,CONNECTOR_ERRORS_DEADLETTERQUEUE_CONTEXT_HEADERS_ENABLE,Optional,MEDIUM,FALSE,"If true, add headers containing error context to the messages written to the dead letter queue. To avoid clashing with headers from the original record, all error context header keys, all error context header keys will start with <code>__connect.errors.</code>"
Connector,CONNECTOR_FORMAT_CLASS,Mandatory,HIGH,,The format class to use when writing data to the store. 
Connector,CONNECTOR_FLUSH_SIZE,Mandatory,HIGH,,Number of records written to store before invoking file commits.
Connector,CONNECTOR_ROTATE_INTERVAL_MS,Optional,HIGH,-1,The time interval in milliseconds to invoke file commits. This configuration ensures that file commits are invoked every configured interval. This configuration is useful when data ingestion rate is low and the connector didn't write enough messages to commit files. The default value -1 means that this feature is disabled.
Connector,CONNECTOR_ROTATE_SCHEDULE_INTERVAL_MS,Optional,MEDIUM,-1,"The time interval in milliseconds to periodically invoke file commits. This configuration ensures that file commits are invoked every configured interval. Time of commit will be adjusted to 00:00 of selected timezone. Commit will be performed at scheduled time regardless previous commit time or number of messages. This configuration is useful when you have to commit your data based on current server time, like at the beginning of every hour. The default value -1 means that this feature is disabled."
Connector,CONNECTOR_SCHEMA_CACHE_SIZE,Optional,LOW,1000,The size of the schema cache used in the Avro converter.
Connector,CONNECTOR_ENHANCED_AVRO_SCHEMA_SUPPORT,Optional,LOW,FALSE,Enable enhanced avro schema support in AvroConverter: Enum symbol preservation and Package Name awareness
Connector,CONNECTOR_CONNECT_META_DATA,Optional,LOW,TRUE,Allow connect converter to add its meta data to the output schema
Connector,CONNECTOR_RETRY_BACKOFF_MS,Optional,LOW,5000,The retry backoff in milliseconds. This config is used to notify Kafka connect to retry delivering a message batch or performing recovery in case of transient exceptions.
Connector,CONNECTOR_FILENAME_OFFSET_ZERO_PAD_WIDTH,Optional,LOW,10,Width to zero-pad offsets in store's filenames if offsets are too short in order to provide fixed-width filenames that can be ordered by simple lexicographic sorting.
Connector,CONNECTOR_AVRO_CODEC,Optional,LOW,null,"The Avro compression codec to be used for output  files. Available values: null, deflate, snappy and bzip2 (CodecSource is org.apache.avro.file.CodecFactory)"
S3,CONNECTOR_S3_BUCKET_NAME,Mandatory,HIGH,,The S3 Bucket.
S3,CONNECTOR_S3_REGION,Optional,MEDIUM,us-west-2,The AWS region to be used the connector.
S3,CONNECTOR_S3_PART_SIZE,Optional,HIGH,26214400,The Part Size in S3 Multi-part Uploads.
S3,CONNECTOR_S3_CREDENTIALS_PROVIDER_CLASS,Optional,LOW,com.amazonaws.auth.DefaultAWSCredentialsProviderChain,Credentials provider or provider chain to use for authentication to AWS. By default the connector uses 'DefaultAWSCredentialsProviderChain'.
S3,CONNECTOR_S3_SSEA_NAME,Optional,LOW,,The S3 Server Side Encryption Algorithm.
S3,CONNECTOR_S3_SSE_CUSTOMER_KEY,Optional,LOW,[hidden],The S3 Server Side Encryption Customer-Provided Key (SSE-C).
S3,CONNECTOR_S3_SSE_KMS_KEY_ID,Optional,LOW,,"The name of the AWS Key Management Service (AWS-KMS) key to be used for server side encryption of the S3 objects. No encryption is used when no key is provided, but it is enabled when 'aws:kms' is specified as encryption algorithm with a valid key name."
S3,CONNECTOR_S3_ACL_CANNED,Optional,LOW,,An S3 canned ACL header value to apply when writing objects.
S3,CONNECTOR_S3_WAN_MODE,Optional,MEDIUM,FALSE,Use S3 accelerated endpoint.
S3,CONNECTOR_S3_COMPRESSION_TYPE,Optional,LOW,none,"Compression type for file written to S3. Applied when using JsonFormat or ByteArrayFormat. Available values: none, gzip."
S3,CONNECTOR_S3_PART_RETRIES,Optional,MEDIUM,3,"Maximum number of retry attempts for failed requests. Zero means no retries. The actual number of attempts is determined by the S3 client based on multiple factors, including, but not limited to - the value of this parameter, type of exception occurred, throttling settings of the underlying S3 client, etc."
S3,CONNECTOR_S3_RETRY_BACKOFF_MS,Optional,LOW,200,"How long to wait in milliseconds before attempting the first retry of a failed S3 request. Upon a failure, this connector may wait up to twice as long as the previous wait, up to the maximum number of retries. This avoids retrying in a tight loop under failure scenarios."
S3,CONNECTOR_FORMAT_BYTEARRAY_EXTENSION,Optional,LOW,.bin,Output file extension for ByteArrayFormat. Defaults to '.bin'
S3,CONNECTOR_FORMAT_BYTEARRAY_SEPARATOR,Optional,LOW,,String inserted between records for ByteArrayFormat. Defaults to 'System.lineSeparator()' and may contain escape sequences like '\n'. An input record that contains the line separator will look like multiple records in the output S3 object.
S3,CONNECTOR_S3_PROXY_URL,Optional,LOW,,S3 Proxy settings encoded in URL syntax. This property is meant to be used only if you need to access S3 through a proxy.
S3,CONNECTOR_S3_PROXY_USER,Optional,LOW,,S3 Proxy User. This property is meant to be used only if you need to access S3 through a proxy. Using ``s3.proxy.user`` instead of embedding the username and password in ``s3.proxy.url`` allows the password to be hidden in the logs.
S3,CONNECTOR_S3_PROXY_PASSWORD,Optional,LOW,[hidden],S3 Proxy Password. This property is meant to be used only if you need to access S3 through a proxy. Using ``s3.proxy.password`` instead of embedding the username and password in ``s3.proxy.url`` allows the password to be hidden in the logs.
Storage,CONNECTOR_STORAGE_CLASS,Mandatory,HIGH,,The underlying storage layer.
Storage,CONNECTOR_TOPICS_DIR,Optional,HIGH,topics,Top level directory to store the data ingested from Kafka.
Storage,CONNECTOR_STORE_URL,Optional,HIGH,,"Store's connection URL, if applicable."
Storage,CONNECTOR_DIRECTORY_DELIM,Optional,MEDIUM,/,Directory delimiter pattern
Storage,CONNECTOR_FILE_DELIM,Optional,MEDIUM,+,File delimiter pattern
Partitioner,CONNECTOR_PARTITIONER_CLASS,Optional,HIGH,io.confluent.connect.storage.partitioner.DefaultPartitioner,"The partitioner to use when writing data to the store. You can use ``DefaultPartitioner``, which preserves the Kafka partitions; ``FieldPartitioner``, which partitions the data to different directories according to the value of the partitioning field specified in ``partition.field.name``; ``TimeBasedPartitioner``, which partitions data according to ingestion time."
Partitioner,CONNECTOR_PARTITION_FIELD_NAME,Optional,MEDIUM,,The name of the partitioning field when FieldPartitioner is used.
Partitioner,CONNECTOR_PARTITION_DURATION_MS,Optional,MEDIUM,-1,The duration of a partition milliseconds used by ``TimeBasedPartitioner``. The default value -1 means that we are not using ``TimeBasedPartitioner``.
Partitioner,CONNECTOR_PATH_FORMAT,Optional,MEDIUM,,"This configuration is used to set the format of the data directories when partitioning with ``TimeBasedPartitioner``. The format set in this configuration converts the Unix timestamp to proper directories strings. For example, if you set ``path.format='year'=YYYY/'month'=MM/'day'=dd/'hour'=HH``, the data directories will have the format ``/year=2015/month=12/day=07/hour=15/``."
Partitioner,CONNECTOR_LOCALE,Optional,MEDIUM,,The locale to use when partitioning with ``TimeBasedPartitioner``.
Partitioner,CONNECTOR_TIMEZONE,Optional,MEDIUM,,The timezone to use when partitioning with ``TimeBasedPartitioner``.
Partitioner,CONNECTOR_TIMESTAMP_EXTRACTOR,Optional,MEDIUM,Wallclock,"The extractor that gets the timestamp for records when partitioning with ``TimeBasedPartitioner``. It can be set to ``Wallclock``, ``Record`` or ``RecordField`` in order to use one of the built-in timestamp extractors or be given the fully-qualified class name of a user-defined class that extends the ``TimestampExtractor`` interface."
Partitioner,CONNECTOR_TIMESTAMP_FIELD,Optional,MEDIUM,timestamp,The record field to be used as timestamp by the timestamp extractor.
Connector,CONNECTOR_SHUTDOWN_TIMEOUT_MS,Optional,MEDIUM,3000,Clean shutdown timeout. This makes sure that asynchronous Hive metastore updates are completed during connector shutdown.
Parquet,CONNECTOR_PARQUET_CODEC,Optional,MEDIUM,snappy,"The Compression codec to be used for parquet files written in s3, Available Options : uncompressed, snappy, gzip, Default Value : snappy"
Parquet,CONNECTOR_PARQUET_PAGE_SIZE,Optional,MEDIUM,1048576,"The page size to be used for the the Parquet files written in s3 in bytes, Default Value : 1048576"
Parquet,CONNECTOR_PARQUET_ROW_GROUP_SIZE,Optional,MEDIUM,134217728,"The Row Group to be used for the the Parquet files written in s3 in bytes, Default Value : 134217728"
Parquet,CONNECTOR_PARQUET_DICTIONARY_PAGE_SIZE,Optional,MEDIUM,1048576,"The Dictionary Page Size to be used for the the Parquet files written in s3 in bytes, Default Value : 1048576"
Parquet,CONNECTOR_PARQUET_MAX_PADDING_SIZE,Optional,MEDIUM,8388608,"The Parquet format max padding size in bytes, that will be used to align  row groups with blocks in the underlying filesystem. If the underlying filesystem is not a block filesystem like HDFS, this has no effect. Default Value : 8388608"
Parquet,CONNECTOR_PARQUET_ENABLE_DICTIONARY,Optional,MEDIUM,TRUE,"Boolean which represents if Dictionary encoding is enabled or not for the parquet files written in s3, Default Value : true"
Parquet,CONNECTOR_PARQUET_ENABLE_VALIDATION,Optional,MEDIUM,FALSE,"Boolean string which represents if the schema validation should be strictly enforced for the parquet files written in s3, Default value : false"
Parquet,CONNECTOR_PARQUET_WRITER_VERSION,Optional,LOW,v1,"The Version of ParquetWriter api to Use. See org.apache.parquet.column.ParquetProperties.WriterVersion, Available Values : v1, v2, Default Value : v1"
Tenant,CONNECTOR_MULTITENANCY_ENABLED,Optional,HIGH,TRUE,"Boolean string which represents if the s3 connector should operate in multi-tenant mode or not, Default Value : trueIf configured 'false' data will be not be segregated by tenants in s3 bucket"
